{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0683072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted class: o\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('keras_model.h5')\n",
    "\n",
    "# Function to preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)  # Convert grayscale to RGB\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "# Function to predict using the model\n",
    "def predict_image(image_path):\n",
    "    # Preprocess the input image\n",
    "    processed_image = preprocess_image(image_path)\n",
    "    \n",
    "    # Perform prediction\n",
    "    prediction = model.predict(processed_image)\n",
    "    \n",
    "    # Decode the prediction (example: convert numerical class to label)\n",
    "    # You need to modify this based on your model's output format\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    # Return the predicted class or label\n",
    "    return predicted_class\n",
    "\n",
    "# Path to the image you want to predict\n",
    "image_path = '0.jpg'\n",
    "\n",
    "# Perform prediction\n",
    "predicted_class = predict_image(image_path)\n",
    "labels = ['a', 'aa', 'i','ii', 'u', 'uu', 'e', 'ee', 'ai', 'o', 'oo', 'au', 'ak']\n",
    "\n",
    "# Print the predicted class or label\n",
    "print(\"Predicted class:\", labels[predicted_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e46dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 1s 781ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 1s 671ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('keras_model.h5')\n",
    "\n",
    "# Function to predict using the model\n",
    "def predict_frame(frame):\n",
    "    # Perform prediction\n",
    "    prediction = model.predict(frame)\n",
    "    \n",
    "    # Decode the prediction (example: convert numerical class to label)\n",
    "    # You need to modify this based on your model's output format\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# Main function to capture video from camera and perform prediction\n",
    "def main():\n",
    "    # Open camera capture\n",
    "    minValue = 70\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        _, frame = capture.read()\n",
    "\n",
    "        # Simulating mirror Image\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Coordinates of the ROI\n",
    "        x1 = int(0.5 * frame.shape[1])\n",
    "        y1 = 10\n",
    "        x2 = frame.shape[1] - 10\n",
    "        y2 = int(0.5 * frame.shape[1])\n",
    "\n",
    "        # Drawing the ROI\n",
    "        cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255, 0, 0), 1)\n",
    "        \n",
    "        # Extracting the ROI\n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Image Processing\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (5, 5), 2)\n",
    "        th3 = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "        ret, test_image = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "        # Resize input image to (224, 224) dimensions\n",
    "        test_image_resized = cv2.resize(test_image, (224, 224))\n",
    "\n",
    "        # Convert resized image to a 3-channel image\n",
    "        test_image_3_channel = cv2.cvtColor(test_image_resized, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Normalize the image\n",
    "        test_image_normalized = test_image_3_channel.astype(np.float32) / 255.0\n",
    "\n",
    "        # Expand dimensions to match model's input shape\n",
    "        test_image_expanded = np.expand_dims(test_image_normalized, axis=0)\n",
    "\n",
    "        # Perform prediction on the preprocessed frame\n",
    "        predicted_class = predict_frame(test_image_expanded)\n",
    "\n",
    "        labels = ['a', 'aa', 'i', 'ii', 'u', 'uu', 'e', 'ee', 'ai', 'o', 'oo', 'au', 'ak']\n",
    "        predicted_label = labels[predicted_class]\n",
    "\n",
    "        # Display the original frame\n",
    "        cv2.imshow('Original Frame', test_image)\n",
    "\n",
    "        # Display the predicted label on the original frame\n",
    "        cv2.putText(frame, predicted_label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the frame with predicted label\n",
    "        cv2.imshow('Frame with Predicted Label', frame)\n",
    "\n",
    "        # Check for exit key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the capture\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040de32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Found 2743 images belonging to 31 classes.\n",
      "Found 684 images belonging to 31 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nandh\\AppData\\Local\\Temp\\ipykernel_20624\\3601422785.py:48: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "86/86 [==============================] - 104s 1s/step - loss: 13.5379 - accuracy: 0.1203 - val_loss: 2.7024 - val_accuracy: 0.2822\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 84s 972ms/step - loss: 1.7249 - accuracy: 0.5680 - val_loss: 0.9663 - val_accuracy: 0.7646\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 77s 892ms/step - loss: 0.4779 - accuracy: 0.8990 - val_loss: 0.3896 - val_accuracy: 0.8991\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 80s 925ms/step - loss: 0.1864 - accuracy: 0.9614 - val_loss: 0.2660 - val_accuracy: 0.9342\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 95s 1s/step - loss: 0.0817 - accuracy: 0.9825 - val_loss: 0.2149 - val_accuracy: 0.9474\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 92s 1s/step - loss: 0.0737 - accuracy: 0.9818 - val_loss: 0.1367 - val_accuracy: 0.9620\n",
      "Epoch 7/10\n",
      "32/86 [==========>...................] - ETA: 49s - loss: 0.0636 - accuracy: 0.9823"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), activation='relu'))\n",
    "\n",
    "# Pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=31, activation='softmax'))  # 31 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Image Data Augmentation\n",
    "data_generator = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)  # Splitting the data into 80% training and 20% validation\n",
    "\n",
    "# Load and augment data from the same directory\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    'dataSet/trainingData',  # Use the same directory for both training and validation\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training')  # Specify subset as 'training' for the training data\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'dataSet/trainingData',  # Use the same directory for both training and validation\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')  # Specify subset as 'validation' for the validation data\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator))\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_model_same_dir.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d000241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('trained_model_same_dir.h5')\n",
    "\n",
    "# Function to predict using the model\n",
    "def predict_frame(frame):\n",
    "    # Perform prediction\n",
    "    prediction = model.predict(frame)\n",
    "    \n",
    "    # Decode the prediction (example: convert numerical class to label)\n",
    "    # You need to modify this based on your model's output format\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# Main function to capture video from camera and perform prediction\n",
    "def main():\n",
    "    # Open camera capture\n",
    "    minValue = 70\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        _, frame = capture.read()\n",
    "\n",
    "        # Simulating mirror Image\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Coordinates of the ROI\n",
    "        x1 = int(0.5 * frame.shape[1])\n",
    "        y1 = 10\n",
    "        x2 = frame.shape[1] - 10\n",
    "        y2 = int(0.5 * frame.shape[1])\n",
    "\n",
    "        # Drawing the ROI\n",
    "        cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255, 0, 0), 1)\n",
    "        \n",
    "        # Extracting the ROI\n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Image Processing\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (5, 5), 2)\n",
    "        th3 = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "        ret, test_image = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "        # Resize input image to (224, 224) dimensions\n",
    "        test_image_resized = cv2.resize(test_image, (224, 224))\n",
    "\n",
    "        # Convert resized image to a 3-channel image\n",
    "        test_image_3_channel = cv2.cvtColor(test_image_resized, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Normalize the image\n",
    "        test_image_normalized = test_image_3_channel.astype(np.float32) / 255.0\n",
    "\n",
    "        # Expand dimensions to match model's input shape\n",
    "        test_image_expanded = np.expand_dims(test_image_normalized, axis=0)\n",
    "\n",
    "        # Perform prediction on the preprocessed frame\n",
    "        predicted_class = predict_frame(test_image_expanded)\n",
    "\n",
    "        labels = ['a', 'aa', 'i', 'ii', 'u', 'uu', 'e', 'ee', 'ai', 'o', 'oo', 'au', 'ak', 'ik', 'ing', 'ich', 'inj', 'it', 'inn', 'ith', 'ind', 'ip', 'im', 'iy', 'ir', 'il', 'iv', 'izh', 'ill', 'irr', 'in']\n",
    "        predicted_label = labels[predicted_class]\n",
    "\n",
    "        # Display the original frame\n",
    "        cv2.imshow('Original Frame', test_image)\n",
    "\n",
    "        # Display the predicted label on the original frame\n",
    "        cv2.putText(frame, predicted_label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the frame with predicted label\n",
    "        cv2.imshow('Frame with Predicted Label', frame)\n",
    "\n",
    "        # Check for exit key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the capture\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e3af8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 1s 628ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 1s 630ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('trained_model_same_dir.h5')\n",
    "\n",
    "# Function to predict using the model\n",
    "def predict_frame(frame):\n",
    "    # Perform prediction\n",
    "    prediction = model.predict(frame)\n",
    "    \n",
    "    # Decode the prediction (example: convert numerical class to label)\n",
    "    # You need to modify this based on your model's output format\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# Main function to capture video from camera and perform prediction\n",
    "def main():\n",
    "    # Open camera capture\n",
    "    minValue = 70\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        _, frame = capture.read()\n",
    "\n",
    "        # Simulating mirror Image\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Coordinates of the ROI\n",
    "        x1 = int(0.5 * frame.shape[1])\n",
    "        y1 = 10\n",
    "        x2 = frame.shape[1] - 10\n",
    "        y2 = int(0.5 * frame.shape[1])\n",
    "\n",
    "        # Drawing the ROI\n",
    "        cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255, 0, 0), 1)\n",
    "        \n",
    "        # Extracting the ROI\n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Image Processing\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (5, 5), 2)\n",
    "        th3 = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "        ret, test_image = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "        # Resize input image to (224, 224) dimensions\n",
    "        test_image_resized = cv2.resize(test_image, (224, 224))\n",
    "\n",
    "        # Convert resized image to a 3-channel image\n",
    "        test_image_3_channel = cv2.cvtColor(test_image_resized, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Normalize the image\n",
    "        test_image_normalized = test_image_3_channel.astype(np.float32) / 255.0\n",
    "\n",
    "        # Expand dimensions to match model's input shape\n",
    "        test_image_expanded = np.expand_dims(test_image_normalized, axis=0)\n",
    "\n",
    "        # Perform prediction on the preprocessed frame\n",
    "        predicted_class = predict_frame(test_image_expanded)\n",
    "\n",
    "        labels = ['a', 'aa', 'i', 'ii', 'u', 'uu', 'e', 'ee', 'ai', 'o', 'oo', 'au', 'ak', 'ik', 'ing', 'ich', 'inj', 'it', 'inn', 'ith', 'ind', 'ip', 'im', 'iy', 'ir', 'il', 'iv', 'izh', 'ill', 'irr', 'in']\n",
    "        predicted_label = labels[predicted_class]\n",
    "\n",
    "        # Display the original frame\n",
    "        cv2.imshow('Original Frame', test_image)\n",
    "\n",
    "        # Display the predicted label on the original frame\n",
    "        cv2.putText(frame, predicted_label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the frame with predicted label\n",
    "        cv2.imshow('Frame with Predicted Label', frame)\n",
    "\n",
    "        # Check for exit key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the capture\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78460672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('trained_model.h5')\n",
    "\n",
    "# Function to preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)  # Convert grayscale to RGB\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "# Function to predict using the model\n",
    "def predict_image(image_path):\n",
    "    # Preprocess the input image\n",
    "    processed_image = preprocess_image(image_path)\n",
    "    \n",
    "    # Perform prediction\n",
    "    prediction = model.predict(processed_image)\n",
    "    \n",
    "    # Decode the prediction (example: convert numerical class to label)\n",
    "    # You need to modify this based on your model's output format\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    # Return the predicted class or label\n",
    "    return predicted_class\n",
    "\n",
    "# Path to the image you want to predict\n",
    "image_path = '0.jpg'\n",
    "\n",
    "# Perform prediction\n",
    "predicted_class = predict_image(image_path)\n",
    "labels = ['a', 'aa', 'i','ii', 'u', 'uu', 'e', 'ee', 'ai', 'o', 'oo', 'au', 'ak']\n",
    "\n",
    "# Print the predicted class or label\n",
    "print(\"Predicted class:\", labels[predicted_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d856f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
