{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f35799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries Required\n",
    "\n",
    "import os\n",
    "import string\n",
    "\n",
    "# Creating the directory Structure\n",
    "\n",
    "if not os.path.exists(\"dataSet\"):\n",
    "    os.makedirs(\"dataSet\")\n",
    "\n",
    "if not os.path.exists(\"dataSet/trainingData\"):\n",
    "    os.makedirs(\"dataSet/trainingData\")\n",
    "\n",
    "if not os.path.exists(\"dataSet/testingData\"):\n",
    "    os.makedirs(\"dataSet/testingData\")\n",
    "\n",
    "# List of Tamil uyir letters\n",
    "uyir_letters = ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ', 'ஃ']\n",
    "\n",
    "# List of Tamil mei letters\n",
    "mei_letters = ['க்', 'ங்', 'ச்', 'ஞ்', 'ட்', 'ண்', 'த்', 'ந்', 'ப்', 'ம்', 'ய்', 'ர்', 'ல்', 'வ்', 'ழ்', 'ள்', 'ற்', 'ன்']\n",
    "\n",
    "for letter in uyir_letters:\n",
    "    folder_path = f\"dataSet/trainingData/{letter}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    folder_path = f\"dataSet/testingData/{letter}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "for letter in mei_letters:\n",
    "    folder_path = f\"dataSet/trainingData/{letter}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    folder_path = f\"dataSet/testingData/{letter}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0abd5427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries Required\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Creating and Collecting Training Data\n",
    "\n",
    "mode = 'trainingData'\n",
    "directory = 'dataSet/' + mode + '/'\n",
    "minValue = 70\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "interrupt = -1\n",
    "\n",
    "transliteration_dict = {\n",
    "    'a': 'அ', 'b': 'ஆ', 'c': 'இ', 'd': 'ஈ', 'e': 'உ', 'f': 'ஊ', 'g': 'எ', 'h': 'ஏ', 'i': 'ஐ', 'j': 'ஒ',\n",
    "    'k': 'ஓ', 'l': 'ஔ', 'm': 'ஃ', 'n': 'க்', 'o': 'ங்', 'p': 'ச்', 'q': 'ஞ்', 'r': 'ட்', 's': 'ண்',\n",
    "    't': 'த்', 'u': 'ந்', 'v': 'ப்', 'w': 'ம்', 'x': 'ய்', 'y': 'ர்', 'z': 'ல்', '0': 'வ்', '1': 'ழ்',\n",
    "    '2': 'ள்', '3': 'ற்', '4': 'ன்'\n",
    "}\n",
    "\n",
    "while True:\n",
    "    _, frame = capture.read()\n",
    "\n",
    "    # Simulating mirror Image\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Getting count of existing images\n",
    "\n",
    "    count = {\n",
    "\n",
    "        # Uyir letters count\n",
    "        'அ': len(os.listdir(directory+\"/அ\")),\n",
    "        'ஆ': len(os.listdir(directory+\"/ஆ\")),\n",
    "        'இ': len(os.listdir(directory+\"/இ\")),\n",
    "        'ஈ': len(os.listdir(directory+\"/ஈ\")),\n",
    "        'உ': len(os.listdir(directory+\"/உ\")),\n",
    "        'ஊ': len(os.listdir(directory+\"/ஊ\")),\n",
    "        'எ': len(os.listdir(directory+\"/எ\")),\n",
    "        'ஏ': len(os.listdir(directory+\"/ஏ\")),\n",
    "        'ஐ': len(os.listdir(directory+\"/ஐ\")),\n",
    "        'ஒ': len(os.listdir(directory+\"/ஒ\")),\n",
    "        'ஓ': len(os.listdir(directory+\"/ஓ\")),\n",
    "        'ஔ': len(os.listdir(directory+\"/ஔ\")),\n",
    "        'ஃ': len(os.listdir(directory+\"/ஃ\")),\n",
    "\n",
    "        # Mei letters count\n",
    "        'க்': len(os.listdir(directory+\"/க்\")),\n",
    "        'ங்': len(os.listdir(directory+\"/ங்\")),\n",
    "        'ச்': len(os.listdir(directory+\"/ச்\")),\n",
    "        'ஞ்': len(os.listdir(directory+\"/ஞ்\")),\n",
    "        'ட்': len(os.listdir(directory+\"/ட்\")),\n",
    "        'ண்': len(os.listdir(directory+\"/ண்\")),\n",
    "        'த்': len(os.listdir(directory+\"/த்\")),\n",
    "        'ந்': len(os.listdir(directory+\"/ந்\")),\n",
    "        'ப்': len(os.listdir(directory+\"/ப்\")),\n",
    "        'ம்': len(os.listdir(directory+\"/ம்\")),\n",
    "        'ய்': len(os.listdir(directory+\"/ய்\")),\n",
    "        'ர்': len(os.listdir(directory+\"/ர்\")),\n",
    "        'ல்': len(os.listdir(directory+\"/ல்\")),\n",
    "        'வ்': len(os.listdir(directory+\"/வ்\")),\n",
    "        'ழ்': len(os.listdir(directory+\"/ழ்\")),\n",
    "        'ள்': len(os.listdir(directory+\"/ள்\")),\n",
    "        'ற்': len(os.listdir(directory+\"/ற்\")),\n",
    "        'ன்': len(os.listdir(directory+\"/ன்\")),\n",
    "    }\n",
    "\n",
    "\n",
    "    # Printing the count of each set on the screen\n",
    "    \n",
    "    cv2.putText(frame, \"a : \" + str(count['அ']), (10, 70), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"aa : \" + str(count['ஆ']), (10, 80), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"i : \" + str(count['இ']), (10, 90), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ii : \" + str(count['ஈ']), (10, 100), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"u : \" + str(count['உ']), (10, 110), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"uu : \" + str(count['ஊ']), (10, 120), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"e : \" + str(count['எ']), (10, 130), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ee : \" + str(count['ஏ']), (10, 140), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ai : \" + str(count['ஐ']), (10, 150), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"o : \" + str(count['ஒ']), (10, 160), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"oo : \" + str(count['ஓ']), (10, 170), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"au : \" + str(count['ஔ']), (10, 180), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ak : \" + str(count['ஃ']), (10, 190), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "\n",
    "    cv2.putText(frame, \"ik : \" + str(count['க்']), (10, 200), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ing : \" + str(count['ங்']), (10, 210), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ich : \" + str(count['ச்']), (10, 220), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"inj : \" + str(count['ஞ்']), (10, 230), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"it : \" + str(count['ட்']), (10, 240), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"in : \" + str(count['ண்']), (10, 250), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ith : \" + str(count['த்']), (10, 260), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ind : \" + str(count['ந்']), (10, 270), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ip : \" + str(count['ப்']), (10, 280), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"im : \" + str(count['ம்']), (10, 290), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"iy : \" + str(count['ய்']), (10, 300), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ir : \" + str(count['ர்']), (10, 310), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"il : \" + str(count['ல்']), (10, 320), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"iv : \" + str(count['வ்']), (10, 330), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"izh : \" + str(count['ழ்']), (10, 340), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ill : \" + str(count['ள்']), (10, 350), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"irr : \" + str(count['ற்']), (10, 360), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"inn : \" + str(count['ன்']), (10, 370), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "\n",
    "    # Coordinates of the ROI\n",
    "    \n",
    "    x1 = int(0.5*frame.shape[1])\n",
    "    y1 = 10\n",
    "    x2 = frame.shape[1]-10\n",
    "    y2 = int(0.5*frame.shape[1])\n",
    "\n",
    "    # Drawing the ROI\n",
    "    # The increment/decrement by 1 is to compensate for the bounding box\n",
    "    \n",
    "    cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)\n",
    "    \n",
    "    # Extracting the ROI\n",
    "    \n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    # Image Processing\n",
    "\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 2)\n",
    "        \n",
    "    th3 = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    ret, test_image = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Output Image after the Image Processing that is used for data collection \n",
    "\n",
    "    test_image = cv2.resize(test_image, (300,300))\n",
    "    cv2.imshow(\"test\", test_image)\n",
    "\n",
    "    # Data Collection\n",
    "\n",
    "    interrupt = cv2.waitKey(10)\n",
    "    if interrupt & 0xFF == 27: \n",
    "        # esc key\n",
    "        break\n",
    "\n",
    "    # Loop through transliteration dictionary\n",
    "    for key, value in transliteration_dict.items():\n",
    "        # Check if the key is pressed\n",
    "        if interrupt & 0xFF == ord(key):\n",
    "            # Save the image with the corresponding letter\n",
    "            cv2.imwrite(directory + value + '/' + str(count[value]) + '.jpg', test_image)\n",
    "        \n",
    "    \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68be180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries Required\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Creating and Collecting Training Data\n",
    "\n",
    "mode = 'testingData'\n",
    "directory = 'dataSet/' + mode + '/'\n",
    "minValue = 35\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "interrupt = -1\n",
    "\n",
    "transliteration_dict = {\n",
    "    'a': 'அ', 'b': 'ஆ', 'c': 'இ', 'd': 'ஈ', 'e': 'உ', 'f': 'ஊ', 'g': 'எ', 'h': 'ஏ', 'i': 'ஐ', 'j': 'ஒ',\n",
    "    'k': 'ஓ', 'l': 'ஔ', 'm': 'ஃ', 'n': 'க்', 'o': 'ங்', 'p': 'ச்', 'q': 'ஞ்', 'r': 'ட்', 's': 'ண்',\n",
    "    't': 'த்', 'u': 'ந்', 'v': 'ப்', 'w': 'ம்', 'x': 'ய்', 'y': 'ர்', 'z': 'ல்', '0': 'வ்', '1': 'ழ்',\n",
    "    '2': 'ள்', '3': 'ற்', '4': 'ன்'\n",
    "}\n",
    "\n",
    "while True:\n",
    "    _, frame = capture.read()\n",
    "\n",
    "    # Simulating mirror Image\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Getting count of existing images\n",
    "\n",
    "    count = {\n",
    "\n",
    "        # Uyir letters count\n",
    "        'அ': len(os.listdir(directory+\"/அ\")),\n",
    "        'ஆ': len(os.listdir(directory+\"/ஆ\")),\n",
    "        'இ': len(os.listdir(directory+\"/இ\")),\n",
    "        'ஈ': len(os.listdir(directory+\"/ஈ\")),\n",
    "        'உ': len(os.listdir(directory+\"/உ\")),\n",
    "        'ஊ': len(os.listdir(directory+\"/ஊ\")),\n",
    "        'எ': len(os.listdir(directory+\"/எ\")),\n",
    "        'ஏ': len(os.listdir(directory+\"/ஏ\")),\n",
    "        'ஐ': len(os.listdir(directory+\"/ஐ\")),\n",
    "        'ஒ': len(os.listdir(directory+\"/ஒ\")),\n",
    "        'ஓ': len(os.listdir(directory+\"/ஓ\")),\n",
    "        'ஔ': len(os.listdir(directory+\"/ஔ\")),\n",
    "        'ஃ': len(os.listdir(directory+\"/ஃ\")),\n",
    "\n",
    "        # Mei letters count\n",
    "        'க்': len(os.listdir(directory+\"/க்\")),\n",
    "        'ங்': len(os.listdir(directory+\"/ங்\")),\n",
    "        'ச்': len(os.listdir(directory+\"/ச்\")),\n",
    "        'ஞ்': len(os.listdir(directory+\"/ஞ்\")),\n",
    "        'ட்': len(os.listdir(directory+\"/ட்\")),\n",
    "        'ண்': len(os.listdir(directory+\"/ண்\")),\n",
    "        'த்': len(os.listdir(directory+\"/த்\")),\n",
    "        'ந்': len(os.listdir(directory+\"/ந்\")),\n",
    "        'ப்': len(os.listdir(directory+\"/ப்\")),\n",
    "        'ம்': len(os.listdir(directory+\"/ம்\")),\n",
    "        'ய்': len(os.listdir(directory+\"/ய்\")),\n",
    "        'ர்': len(os.listdir(directory+\"/ர்\")),\n",
    "        'ல்': len(os.listdir(directory+\"/ல்\")),\n",
    "        'வ்': len(os.listdir(directory+\"/வ்\")),\n",
    "        'ழ்': len(os.listdir(directory+\"/ழ்\")),\n",
    "        'ள்': len(os.listdir(directory+\"/ள்\")),\n",
    "        'ற்': len(os.listdir(directory+\"/ற்\")),\n",
    "        'ன்': len(os.listdir(directory+\"/ன்\")),\n",
    "    }\n",
    "\n",
    "    # Printing the count of each set on the screen\n",
    "    \n",
    "    cv2.putText(frame, \"a : \" + str(count['அ']), (10, 70), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"aa : \" + str(count['ஆ']), (10, 80), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"i : \" + str(count['இ']), (10, 90), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ii : \" + str(count['ஈ']), (10, 100), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"u : \" + str(count['உ']), (10, 110), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"uu : \" + str(count['ஊ']), (10, 120), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"e : \" + str(count['எ']), (10, 130), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ee : \" + str(count['ஏ']), (10, 140), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ai : \" + str(count['ஐ']), (10, 150), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"o : \" + str(count['ஒ']), (10, 160), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"oo : \" + str(count['ஓ']), (10, 170), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"au : \" + str(count['ஔ']), (10, 180), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ak : \" + str(count['ஃ']), (10, 190), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "\n",
    "    cv2.putText(frame, \"ik : \" + str(count['க்']), (10, 200), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ing : \" + str(count['ங்']), (10, 210), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ich : \" + str(count['ச்']), (10, 220), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"inj : \" + str(count['ஞ்']), (10, 230), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"it : \" + str(count['ட்']), (10, 240), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"in : \" + str(count['ண்']), (10, 250), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ith : \" + str(count['த்']), (10, 260), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ind : \" + str(count['ந்']), (10, 270), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ip : \" + str(count['ப்']), (10, 280), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"im : \" + str(count['ம்']), (10, 290), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"iy : \" + str(count['ய்']), (10, 300), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ir : \" + str(count['ர்']), (10, 310), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"il : \" + str(count['ல்']), (10, 320), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"iv : \" + str(count['வ்']), (10, 330), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"izh : \" + str(count['ழ்']), (10, 340), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"ill : \" + str(count['ள்']), (10, 350), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"irr : \" + str(count['ற்']), (10, 360), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "    cv2.putText(frame, \"inn : \" + str(count['ன்']), (10, 370), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n",
    "\n",
    "    # Coordinates of the ROI\n",
    "    \n",
    "    x1 = int(0.5*frame.shape[1])\n",
    "    y1 = 10\n",
    "    x2 = frame.shape[1]-10\n",
    "    y2 = int(0.5*frame.shape[1])\n",
    "\n",
    "    # Drawing the ROI\n",
    "    # The increment/decrement by 1 is to compensate for the bounding box\n",
    "    \n",
    "    cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)\n",
    "    \n",
    "    # Extracting the ROI\n",
    "    \n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    # Image Processing\n",
    "\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),2)\n",
    "        \n",
    "    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "    ret, test_image = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Output Image after the Image Processing that is used for data collection \n",
    "\n",
    "    test_image = cv2.resize(test_image, (300,300))\n",
    "    cv2.imshow(\"test\", test_image)\n",
    "\n",
    "    # Data Collection\n",
    "\n",
    "    interrupt = cv2.waitKey(10)\n",
    "    if interrupt & 0xFF == 27: \n",
    "        # esc key\n",
    "        break\n",
    "\n",
    "    # Loop through transliteration dictionary\n",
    "    for key, value in transliteration_dict.items():\n",
    "        # Check if the key is pressed\n",
    "        if interrupt & 0xFF == ord(key):\n",
    "            # Save the image with the corresponding letter\n",
    "            cv2.imwrite(directory + value + '/' + str(count[value]) + '.jpg', test_image)        \n",
    "    \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a535b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 3422 images belonging to 31 classes.\n",
      "Found 1860 images belonging to 31 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "100/100 [==============================] - 119s 1s/step - loss: 3.1785 - accuracy: 0.0835 - val_loss: 3.0436 - val_accuracy: 0.1781\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 113s 1s/step - loss: 2.1444 - accuracy: 0.3280 - val_loss: 1.6621 - val_accuracy: 0.4663\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 1.4846 - accuracy: 0.5097 - val_loss: 1.4167 - val_accuracy: 0.5138\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 117s 1s/step - loss: 1.1206 - accuracy: 0.6226 - val_loss: 1.5496 - val_accuracy: 0.4938\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.9072 - accuracy: 0.6936 - val_loss: 1.3823 - val_accuracy: 0.5975\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 116s 1s/step - loss: 0.7558 - accuracy: 0.7420 - val_loss: 2.8153 - val_accuracy: 0.4606\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.6319 - accuracy: 0.7855 - val_loss: 1.5858 - val_accuracy: 0.6712\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.5685 - accuracy: 0.8089 - val_loss: 1.6447 - val_accuracy: 0.6175\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'dataSet/trainingData',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'dataSet/testingData',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631bb5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3422 images belonging to 31 classes.\n",
      "Found 1860 images belonging to 31 classes.\n",
      "Found 1860 images belonging to 31 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nandh\\AppData\\Local\\Temp\\ipykernel_17752\\1669006585.py:40: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=len(train_generator), epochs=10, validation_data=validation_generator, validation_steps=len(validation_generator))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - 89s 818ms/step - loss: 10.6850 - accuracy: 0.0894 - val_loss: 3.4971 - val_accuracy: 0.0812\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 130s 1s/step - loss: 1.9834 - accuracy: 0.4413 - val_loss: 3.9688 - val_accuracy: 0.2462\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 231s 2s/step - loss: 0.6704 - accuracy: 0.8238 - val_loss: 4.9617 - val_accuracy: 0.2602\n",
      "Epoch 4/10\n",
      " 45/107 [===========>..................] - ETA: 1:44 - loss: 0.3089 - accuracy: 0.9242"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), activation='relu'))\n",
    "\n",
    "# Pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=31, activation='softmax'))  # 31 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Image Data Augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load and augment training data\n",
    "train_generator = train_datagen.flow_from_directory('dataSet/trainingData', target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Load validation data\n",
    "validation_generator = test_datagen.flow_from_directory('dataSet/testingData', target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Load and preprocess test data\n",
    "test_generator = test_datagen.flow_from_directory('dataSet/testingData', target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=len(train_generator), epochs=10, validation_data=validation_generator, validation_steps=len(validation_generator))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate_generator(test_generator, steps=len(test_generator))\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70de42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3422 images belonging to 31 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nandh\\AppData\\Local\\Temp\\ipykernel_13980\\1306687645.py:39: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - 162s 1s/step - loss: 2.5051 - accuracy: 0.5368\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 188s 2s/step - loss: 0.1522 - accuracy: 0.9576\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 251s 2s/step - loss: 0.0582 - accuracy: 0.9848\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 295s 3s/step - loss: 0.0445 - accuracy: 0.9874\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 299s 3s/step - loss: 0.0291 - accuracy: 0.9909\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 359s 3s/step - loss: 0.0280 - accuracy: 0.9927\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 365s 3s/step - loss: 0.0098 - accuracy: 0.9977\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 373s 3s/step - loss: 0.0245 - accuracy: 0.9906\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 342s 3s/step - loss: 0.0045 - accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 337s 3s/step - loss: 0.0205 - accuracy: 0.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=31, activation='softmax'))  # 13 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Image Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Load and augment training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'dataSet/trainingData',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def31712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
