{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea6b422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 998ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "from PIL import Image, ImageTk\n",
    "import math\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Button\n",
    "\n",
    "class Application:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Indian Sign Language Recognition\")\n",
    "        self.root.geometry(\"1260x700\")\n",
    "        self.root.config(background=\"#11754E\")\n",
    "        self.create_widgets()\n",
    "\n",
    "        # Initialize hand detector and classifier\n",
    "        self.detector = HandDetector(maxHands=1)\n",
    "        self.classifier = Classifier(\"keras_model.h5\", \"labels.txt\")\n",
    "\n",
    "        # Initialize video capture\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.offset = 20\n",
    "        self.imgSize = 300\n",
    "        self.labels = ['Bathroom', 'Break', 'Call', 'Cute', 'Dad', 'Dislike', 'Drink', 'Fighting', 'Food', 'Happy', 'Hello', 'I Love You', 'Its Me', 'Like', 'Milk', 'Mom', 'Name', 'No', 'Ok', 'Peace', 'Promise', 'Ready', 'Saying', 'Sibling', 'Silence', 'Stop', 'Terrific', 'Thank You', 'Water', 'Yes']\n",
    "        self.predicted_word = \"\"\n",
    "\n",
    "        # Start video loop\n",
    "        self.video_loop()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.panel = Label(self.root)\n",
    "        self.panel.place(x=135, y=10, width=640, height=480)\n",
    "        self.panel.config(bg=\"#E0E0E0\")\n",
    "\n",
    "        self.panel2 = Label(self.root)\n",
    "        self.panel2.place(x=950, y=20, width=310, height=310)\n",
    "        self.panel2.config(bg=\"#E0E0E0\")\n",
    "\n",
    "        self.T2 = Label(self.root, text=\"Word :\", font=(\"Comic Sans MS\", 16, \"bold\"), bg=\"#11754E\", fg=\"white\")\n",
    "        self.T2.place(x=145, y=600)\n",
    "\n",
    "        self.word_label = Label(self.root, text=\"\", font=(\"Comic Sans MS\", 16), bg=\"#11754E\", fg=\"white\")\n",
    "        self.word_label.place(x=250, y=600)\n",
    "\n",
    "        speak_button = Button(self.root, text=\"Speak\", font=(\"Comic Sans MS\", 10, \"bold\"), bg=\"#E48F1B\", fg=\"#34262B\", command=self.speak_word)\n",
    "        speak_button.place(x=500, y=600)\n",
    "\n",
    "        clear_button = Button(self.root, text=\"Clear\", font=(\"Comic Sans MS\", 10, \"bold\"), bg=\"#E48F1B\", fg=\"#34262B\", command=self.clear_word)\n",
    "        clear_button.place(x=600, y=600)\n",
    "\n",
    "    def video_loop(self):\n",
    "        success, img = self.cap.read()\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = self.detector.findHands(img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "\n",
    "            imgWhite = np.ones((self.imgSize, self.imgSize, 3), np.uint8) * 255\n",
    "            imgCrop = img[y - self.offset:y + h + self.offset, x - self.offset:x + w + self.offset]\n",
    "\n",
    "            imgCropShape = imgCrop.shape\n",
    "\n",
    "            aspectRatio = h / w\n",
    "\n",
    "            if aspectRatio > 1:\n",
    "                k = self.imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, self.imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((self.imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "                prediction, index = self.classifier.getPrediction(imgWhite, draw=False)\n",
    "                self.predicted_word = self.labels[index]\n",
    "\n",
    "            else:\n",
    "                k = self.imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (self.imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((self.imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "                prediction, index = self.classifier.getPrediction(imgWhite, draw=False)\n",
    "                self.predicted_word = self.labels[index]\n",
    "\n",
    "            self.word_label.config(text=self.predicted_word)\n",
    "\n",
    "            cv2.rectangle(imgOutput, (x - self.offset, y - self.offset - 50),\n",
    "                          (x - self.offset + 90, y - self.offset - 50 + 50), (255, 0, 255), cv2.FILLED)\n",
    "            cv2.putText(imgOutput, self.predicted_word, (x, y - 26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x - self.offset, y - self.offset),\n",
    "                          (x + w + self.offset, y + h + self.offset), (255, 0, 255), 4)\n",
    "\n",
    "        imgOutput = cv2.cvtColor(imgOutput, cv2.COLOR_BGR2RGB)\n",
    "        imgOutput = cv2.resize(imgOutput, (640, 480))\n",
    "\n",
    "        imgOutput = Image.fromarray(imgOutput)\n",
    "        imgtk = ImageTk.PhotoImage(image=imgOutput)\n",
    "        self.panel.imgtk = imgtk\n",
    "        self.panel.config(image=imgtk)\n",
    "\n",
    "        self.root.after(10, self.video_loop)\n",
    "\n",
    "\n",
    "    def speak_word(self):\n",
    "        # Add code to generate audio for the predicted word (self.predicted_word)\n",
    "        pass\n",
    "\n",
    "    def clear_word(self):\n",
    "        self.word_label.config(text=\"\")\n",
    "        self.predicted_word = \"\"\n",
    "\n",
    "root = tk.Tk()\n",
    "app = Application(root)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e8f0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\nandh\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "from PIL import Image, ImageTk\n",
    "import math\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Button\n",
    "\n",
    "class Application:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Indian Sign Language Recognition\")\n",
    "        self.root.geometry(\"1260x700\")\n",
    "        self.root.config(background=\"#11754E\")\n",
    "        self.create_widgets()\n",
    "\n",
    "        # Initialize hand detector and classifier\n",
    "        self.detector = HandDetector(maxHands=1)\n",
    "        self.classifier = Classifier(\"Model/uyir_t_model.h5\", \"Model/u_t_labels.txt\")\n",
    "\n",
    "        # Initialize video capture\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.offset = 20\n",
    "        self.imgSize = 300\n",
    "        self.labels = ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ', 'ஃ']\n",
    "        #self.labels = ['Bathroom', 'Break', 'Call', 'Cute', 'Dad', 'Dislike', 'Drink', 'Fighting', 'Food', 'Happy', 'Hello', 'I Love You', 'Its Me', 'Like', 'Milk', 'Mom', 'Name', 'No', 'Ok', 'Peace', 'Promise', 'Ready', 'Saying', 'Sibling', 'Silence', 'Stop', 'Terrific', 'Thank You', 'Water', 'Yes']\n",
    "        self.predicted_word = \"\"\n",
    "\n",
    "        # Start video loop\n",
    "        self.video_loop()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.panel = Label(self.root)\n",
    "        self.panel.place(x=135, y=10, width=640, height=480)\n",
    "        self.panel.config(bg=\"#E0E0E0\")\n",
    "\n",
    "        self.panel2 = Label(self.root)\n",
    "        self.panel2.place(x=950, y=20, width=310, height=310)\n",
    "        self.panel2.config(bg=\"#E0E0E0\")\n",
    "\n",
    "        self.T2 = Label(self.root, text=\"Word :\", font=(\"Comic Sans MS\", 16, \"bold\"), bg=\"#11754E\", fg=\"white\")\n",
    "        self.T2.place(x=145, y=600)\n",
    "\n",
    "        self.word_label = Label(self.root, text=\"\", font=(\"Comic Sans MS\", 16), bg=\"#11754E\", fg=\"white\")\n",
    "        self.word_label.place(x=250, y=600)\n",
    "\n",
    "        speak_button = Button(self.root, text=\"Speak\", font=(\"Comic Sans MS\", 10, \"bold\"), bg=\"#E48F1B\", fg=\"#34262B\", command=self.speak_word)\n",
    "        speak_button.place(x=500, y=600)\n",
    "\n",
    "        clear_button = Button(self.root, text=\"Clear\", font=(\"Comic Sans MS\", 10, \"bold\"), bg=\"#E48F1B\", fg=\"#34262B\", command=self.clear_word)\n",
    "        clear_button.place(x=600, y=600)\n",
    "\n",
    "    def video_loop(self):\n",
    "        success, img = self.cap.read()\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = self.detector.findHands(img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "\n",
    "            imgWhite = np.ones((self.imgSize, self.imgSize, 3), np.uint8) * 255\n",
    "            imgCrop = img[y - self.offset:y + h + self.offset, x - self.offset:x + w + self.offset]\n",
    "\n",
    "            imgCropShape = imgCrop.shape\n",
    "\n",
    "            aspectRatio = h / w\n",
    "\n",
    "            if aspectRatio > 1:\n",
    "                k = self.imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, self.imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((self.imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "                prediction, index = self.classifier.getPrediction(imgWhite, draw=False)\n",
    "                self.predicted_word = self.labels[index]\n",
    "\n",
    "            else:\n",
    "                k = self.imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (self.imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((self.imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "                prediction, index = self.classifier.getPrediction(imgWhite, draw=False)\n",
    "                self.predicted_word = self.labels[index]\n",
    "\n",
    "            self.word_label.config(text=self.predicted_word)\n",
    "\n",
    "            cv2.rectangle(imgOutput, (x - self.offset, y - self.offset - 50),\n",
    "                          (x - self.offset + 90, y - self.offset - 50 + 50), (255, 0, 255), cv2.FILLED)\n",
    "            cv2.putText(imgOutput, self.predicted_word, (x, y - 26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x - self.offset, y - self.offset),\n",
    "                          (x + w + self.offset, y + h + self.offset), (255, 0, 255), 4)\n",
    "\n",
    "            # Display resized hand image on panel2\n",
    "            imgResize = cv2.cvtColor(imgResize, cv2.COLOR_BGR2RGB)\n",
    "            imgResize = Image.fromarray(imgResize)\n",
    "            imgtk = ImageTk.PhotoImage(image=imgResize)\n",
    "            self.panel2.imgtk = imgtk\n",
    "            self.panel2.config(image=imgtk)\n",
    "\n",
    "        imgOutput = cv2.cvtColor(imgOutput, cv2.COLOR_BGR2RGB)\n",
    "        imgOutput = cv2.resize(imgOutput, (640, 480))\n",
    "\n",
    "        imgOutput = Image.fromarray(imgOutput)\n",
    "        imgtk = ImageTk.PhotoImage(image=imgOutput)\n",
    "        self.panel.imgtk = imgtk\n",
    "        self.panel.config(image=imgtk)\n",
    "\n",
    "        self.root.after(10, self.video_loop)\n",
    "\n",
    "\n",
    "    def speak_word(self):\n",
    "        # Add code to generate audio for the predicted word (self.predicted_word)\n",
    "        pass\n",
    "\n",
    "    def clear_word(self):\n",
    "        self.word_label.config(text=\"\")\n",
    "        self.predicted_word = \"\"\n",
    "\n",
    "root = tk.Tk()\n",
    "app = Application(root)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc47ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 923ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nandh\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\nandh\\anaconda3\\lib\\tkinter\\__init__.py\", line 814, in callit\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\nandh\\AppData\\Local\\Temp\\ipykernel_22740\\313348662.py\", line 101, in video_loop\n",
      "    imgResize = cv2.resize(imgCrop, (self.imgSize, hCal))\n",
      "cv2.error: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "from PIL import Image, ImageTk\n",
    "import math\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Button, Frame\n",
    "\n",
    "class Application:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Sign Language Interpreter\")\n",
    "        self.root.geometry(\"1260x700\")\n",
    "        self.root.config(background=\"#001F3F\")  # Dark navy blue background\n",
    "        self.create_widgets()\n",
    "\n",
    "        # Initialize hand detector and classifier\n",
    "        self.detector = HandDetector(maxHands=1)\n",
    "        self.classifier = Classifier(\"Model/uyir_t_model.h5\", \"Model/u_t_labels.txt\")\n",
    "\n",
    "        # Initialize video capture\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.offset = 20\n",
    "        self.imgSize = 300\n",
    "        self.labels = ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ', 'ஃ']\n",
    "        self.predicted_word = \"\"\n",
    "\n",
    "        # Start video loop\n",
    "        self.video_loop()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        # Create a frame to hold the title label\n",
    "        title_frame = Frame(self.root, bg=\"#001F3F\")\n",
    "        title_frame.pack(fill=tk.X, padx=20, pady=(20, 0))\n",
    "\n",
    "        # Title label\n",
    "        self.title_label = Label(title_frame, text=\"Sign Language Interpreter\", font=(\"Comic Sans MS\", 24, \"bold\"), bg=\"#001F3F\", fg=\"white\")\n",
    "        self.title_label.pack(pady=20)\n",
    "\n",
    "        # Create a frame to hold the panels\n",
    "        panels_frame = Frame(self.root, bg=\"#001F3F\")\n",
    "        panels_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "\n",
    "        # Left panel\n",
    "        self.panel = Label(panels_frame)\n",
    "        self.panel.pack(side=tk.LEFT, padx=20)\n",
    "\n",
    "        # Right panel\n",
    "        self.panel2 = Label(panels_frame)\n",
    "        self.panel2.pack(side=tk.LEFT, padx=20)\n",
    "\n",
    "        # Widgets below the panels\n",
    "        self.bottom_frame = Frame(self.root, bg=\"#001F3F\")\n",
    "        self.bottom_frame.pack(fill=tk.X, padx=20, pady=20)\n",
    "\n",
    "        self.T2 = Label(self.bottom_frame, text=\"Word :\", font=(\"Comic Sans MS\", 16, \"bold\"), bg=\"#001F3F\", fg=\"white\")\n",
    "        self.T2.grid(row=0, column=0, pady=(20, 0), padx=(20, 0), sticky=\"w\")\n",
    "\n",
    "        self.word_label = Label(self.bottom_frame, text=\"\", font=(\"Comic Sans MS\", 16), bg=\"#001F3F\", fg=\"white\")\n",
    "        self.word_label.grid(row=0, column=1, pady=(20, 0), padx=(10, 0), sticky=\"w\")\n",
    "\n",
    "        speak_button = Button(self.bottom_frame, text=\"Speak\", font=(\"Comic Sans MS\", 10, \"bold\"), bg=\"#E48F1B\", fg=\"#34262B\", command=self.speak_word)\n",
    "        speak_button.grid(row=0, column=2, pady=(20, 0), padx=(10, 20), sticky=\"w\")\n",
    "\n",
    "        clear_button = Button(self.bottom_frame, text=\"Clear\", font=(\"Comic Sans MS\", 10, \"bold\"), bg=\"#E48F1B\", fg=\"#34262B\", command=self.clear_word)\n",
    "        clear_button.grid(row=0, column=3, pady=(20, 0), padx=(0, 20), sticky=\"w\")\n",
    "\n",
    "    def video_loop(self):\n",
    "        success, img = self.cap.read()\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = self.detector.findHands(img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "\n",
    "            imgWhite = np.ones((self.imgSize, self.imgSize, 3), np.uint8) * 255\n",
    "            imgCrop = img[y - self.offset:y + h + self.offset, x - self.offset:x + w + self.offset]\n",
    "\n",
    "            imgCropShape = imgCrop.shape\n",
    "\n",
    "            aspectRatio = h / w\n",
    "\n",
    "            if aspectRatio > 1:\n",
    "                k = self.imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, self.imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((self.imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "                prediction, index = self.classifier.getPrediction(imgWhite, draw=False)\n",
    "                self.predicted_word = self.labels[index]\n",
    "\n",
    "            else:\n",
    "                k = self.imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (self.imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((self.imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "                prediction, index = self.classifier.getPrediction(imgWhite, draw=False)\n",
    "                self.predicted_word = self.labels[index]\n",
    "\n",
    "            self.word_label.config(text=self.predicted_word)\n",
    "\n",
    "            cv2.rectangle(imgOutput, (x - self.offset, y - self.offset - 50),\n",
    "                          (x - self.offset + 90, y - self.offset - 50 + 50), (255, 0, 255), cv2.FILLED)\n",
    "            cv2.putText(imgOutput, self.predicted_word, (x, y - 26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x - self.offset, y - self.offset),\n",
    "                          (x + w + self.offset, y + h + self.offset), (255, 0, 255), 4)\n",
    "\n",
    "            # Display resized hand image on panel2\n",
    "            imgResize = cv2.cvtColor(imgResize, cv2.COLOR_BGR2RGB)\n",
    "            imgResize = Image.fromarray(imgResize)\n",
    "            imgtk = ImageTk.PhotoImage(image=imgResize)\n",
    "            self.panel2.imgtk = imgtk\n",
    "            self.panel2.config(image=imgtk)\n",
    "\n",
    "        imgOutput = cv2.cvtColor(imgOutput, cv2.COLOR_BGR2RGB)\n",
    "        imgOutput = cv2.resize(imgOutput, (640, 480))\n",
    "\n",
    "        imgOutput = Image.fromarray(imgOutput)\n",
    "        imgtk = ImageTk.PhotoImage(image=imgOutput)\n",
    "        self.panel.imgtk = imgtk\n",
    "        self.panel.config(image=imgtk)\n",
    "\n",
    "        self.root.after(10, self.video_loop)\n",
    "\n",
    "\n",
    "    def speak_word(self):\n",
    "        # Add code to generate audio for the predicted word (self.predicted_word)\n",
    "        pass\n",
    "\n",
    "    def clear_word(self):\n",
    "        self.word_label.config(text=\"\")\n",
    "        self.predicted_word = \"\"\n",
    "\n",
    "root = tk.Tk()\n",
    "app = Application(root)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a9a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
